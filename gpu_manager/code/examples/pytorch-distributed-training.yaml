# pytorch-distributed-training.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pytorch-distributed-training
spec:
  parallelism: 4  # 4个并行训练任务
  template:
    spec:
      containers:
      - name: pytorch-worker
        image: pytorch/pytorch:1.12.0-cuda11.3-cudnn8-runtime
        resources:
          limits:
            nvidia.com/gpu: "1"        # 每个worker使用1个vGPU
            nvidia.com/gpumem: "6000"   # 分配6GB显存
            nvidia.com/gpucores: "75"   # 使用75%计算资源
        env:
        - name: MASTER_ADDR
          value: "pytorch-master"
        - name: MASTER_PORT
          value: "29500"
        - name: WORLD_SIZE
          value: "4"
        command:
        - python
        - -m
        - torch.distributed.launch
        - --nproc_per_node=1
        - train_distributed.py
        volumeMounts:
        - name: training-data
          mountPath: /data
        - name: model-output
          mountPath: /output